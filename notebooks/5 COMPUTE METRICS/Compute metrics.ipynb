{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T20:58:20.423898Z",
     "start_time": "2024-03-11T20:58:15.193483Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T20:58:20.455896Z",
     "start_time": "2024-03-11T20:58:20.424901Z"
    }
   },
   "id": "d15947033864af6d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TextRecognizePipeline:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ocr_processor = TrOCRProcessor.from_pretrained(\"raxtemur/trocr-base-ru\")\n",
    "        self.ocr_model = VisionEncoderDecoderModel.from_pretrained(path_to_checkpoint, local_files_only=True).to(device)\n",
    "        self.detection_model = YOLO(\"../../models/new_text_detector/best_1024.pt\").to(device)\n",
    "        \n",
    "    \n",
    "    def prepare_orc_model(self):\n",
    "        # Set special tokens used for creating the decoder_input_ids from the labels.\n",
    "        self.ocr_model.config.decoder_start_token_id = self.ocr_processor.tokenizer.cls_token_id\n",
    "        self.ocr_model.config.pad_token_id = self.ocr_processor.tokenizer.pad_token_id\n",
    "        # Set Correct vocab size.\n",
    "        self.ocr_model.config.vocab_size = self.ocr_model.config.decoder.vocab_size\n",
    "        self.ocr_model.config.eos_token_id = self.ocr_processor.tokenizer.sep_token_id\n",
    "        \n",
    "        self.ocr_model.config.max_length = 64\n",
    "        self.ocr_model.config.early_stopping = True\n",
    "        self.ocr_model.config.no_repeat_ngram_size = 3\n",
    "        self.ocr_model.config.length_penalty = 2.0\n",
    "        self.ocr_model.config.num_beams = 4\n",
    "    \n",
    "    def get_detections_and_crop_boxes(self, img_list: list[Image]) -> list[Image]:\n",
    "        result = []\n",
    "        for predict, image in zip(self.detection_model.predict(img_list), img_list):\n",
    "            for box in predict.boxes.xyxy.cpu().tolist():\n",
    "                cropped_image = image.crop(box)\n",
    "                result.append(cropped_image)\n",
    "        return result\n",
    "    \n",
    "    def get_ocr_predictions(self, img_list: list[Image]):\n",
    "        pixel_values = self.ocr_processor(img_list, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        generated_ids = self.ocr_model.generate(pixel_values)\n",
    "        generated_text = self.ocr_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        return generated_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T22:26:08.228661Z",
     "start_time": "2024-03-11T22:26:08.219614Z"
    }
   },
   "id": "a19570a706ccbd98",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ocr_pipeline = TextRecognizePipeline()\n",
    "ocr_pipeline.prepare_orc_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T22:26:13.421727Z",
     "start_time": "2024-03-11T22:26:08.904932Z"
    }
   },
   "id": "665832d634a749c9",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1024x736 10 texts, 21.0ms\n",
      "Speed: 4.0ms preprocess, 21.0ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 736)\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(get_rand_image())\n",
    "img = ImageOps.exif_transpose(img)\n",
    "\n",
    "t = ocr_pipeline.get_detections_and_crop_boxes([img])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T22:26:13.843657Z",
     "start_time": "2024-03-11T22:26:13.422728Z"
    }
   },
   "id": "92e8cabefb977b7a",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['поковскаго ухода Давыдовелою Волос',\n 'Волостной Старшина Лаврейти Дми',\n 'трiевъ. 1 На подлинномъ печать сеня',\n 'Генералъ-Мазоръ',\n 'тнаго Старшины.',\n 'Свиты Его Величества',\n 'Съ ходяйствомъ верно',\n 'Председатель Крисутствiя',\n '№ 4.',\n '№ 11.']"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_pipeline.get_ocr_predictions(t)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T22:26:20.452920Z",
     "start_time": "2024-03-11T22:26:15.198514Z"
    }
   },
   "id": "3465e184b646a5a6",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "path_to_checkpoint = \"../../models/text_recognizer/checkpoint-1107/\"\n",
    "\n",
    "# Инициализируем распознавание текста\n",
    "processor = TrOCRProcessor.from_pretrained(\"raxtemur/trocr-base-ru\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(path_to_checkpoint, local_files_only=True)\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3ca83a3f31e3bbc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "text_detector = YOLO(\"../../models/new_text_detector/best_1024.pt\").to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T20:58:34.376376Z",
     "start_time": "2024-03-11T20:58:34.191274Z"
    }
   },
   "id": "81c98bcdefb443fa",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def get_rand_image():\n",
    "    path = pathlib.Path(\"../../data/raw/Распознавание текстов/Уставные грамоты в jpg (Просветов)\")\n",
    "    return np.random.choice(list(path.iterdir()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T20:58:39.457685Z",
     "start_time": "2024-03-11T20:58:39.441685Z"
    }
   },
   "id": "6dfe665a9f58cd05",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1024x768 24 texts, 23.2ms\n",
      "Speed: 5.5ms preprocess, 23.2ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 768)\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "conf: tensor([0.9534, 0.9532, 0.9523, 0.9519, 0.9504, 0.9502, 0.9485, 0.9460, 0.9447, 0.9446, 0.9442, 0.9423, 0.9407, 0.9401, 0.9388, 0.9377, 0.9363, 0.9341, 0.9272, 0.9268, 0.9246, 0.9179, 0.9057, 0.8957], device='cuda:0')\n",
      "data: tensor([[1.0748e+03, 3.2288e+03, 3.2465e+03, 3.4166e+03, 9.5341e-01, 0.0000e+00],\n",
      "        [1.0803e+03, 3.1040e+03, 3.2587e+03, 3.2751e+03, 9.5324e-01, 0.0000e+00],\n",
      "        [1.1057e+03, 1.8397e+03, 3.2291e+03, 2.0472e+03, 9.5235e-01, 0.0000e+00],\n",
      "        [1.0423e+03, 5.0129e+02, 3.2115e+03, 7.5331e+02, 9.5191e-01, 0.0000e+00],\n",
      "        [1.0960e+03, 2.5498e+03, 3.2448e+03, 2.7314e+03, 9.5035e-01, 0.0000e+00],\n",
      "        [1.0737e+03, 2.3964e+03, 3.1750e+03, 2.6070e+03, 9.5021e-01, 0.0000e+00],\n",
      "        [1.0532e+03, 3.6519e+03, 3.2511e+03, 3.8137e+03, 9.4849e-01, 0.0000e+00],\n",
      "        [1.0934e+03, 2.9028e+03, 3.2537e+03, 3.0788e+03, 9.4598e-01, 0.0000e+00],\n",
      "        [1.0995e+03, 2.1397e+03, 3.2382e+03, 2.3431e+03, 9.4467e-01, 0.0000e+00],\n",
      "        [1.0790e+03, 1.3236e+03, 3.2459e+03, 1.5249e+03, 9.4461e-01, 0.0000e+00],\n",
      "        [1.0561e+03, 6.3794e+02, 3.1599e+03, 8.3916e+02, 9.4424e-01, 0.0000e+00],\n",
      "        [1.0858e+03, 1.7637e+03, 3.0955e+03, 1.9156e+03, 9.4226e-01, 0.0000e+00],\n",
      "        [1.0524e+03, 3.4834e+03, 3.2452e+03, 3.6920e+03, 9.4073e-01, 0.0000e+00],\n",
      "        [1.0691e+03, 1.1055e+03, 3.2153e+03, 1.2381e+03, 9.4005e-01, 0.0000e+00],\n",
      "        [1.0744e+03, 1.5038e+03, 3.2479e+03, 1.6403e+03, 9.3878e-01, 0.0000e+00],\n",
      "        [1.0876e+03, 2.8071e+03, 3.1725e+03, 2.9956e+03, 9.3765e-01, 0.0000e+00],\n",
      "        [1.0805e+03, 1.6417e+03, 3.2512e+03, 1.7824e+03, 9.3630e-01, 0.0000e+00],\n",
      "        [1.0699e+03, 9.7850e+02, 3.0674e+03, 1.1071e+03, 9.3408e-01, 0.0000e+00],\n",
      "        [1.0506e+03, 3.3863e+03, 3.2504e+03, 3.5575e+03, 9.2722e-01, 0.0000e+00],\n",
      "        [1.0743e+03, 8.3068e+02, 3.1011e+03, 9.7145e+02, 9.2683e-01, 0.0000e+00],\n",
      "        [1.1138e+03, 2.0367e+03, 3.1278e+03, 2.2043e+03, 9.2464e-01, 0.0000e+00],\n",
      "        [1.0902e+03, 2.3185e+03, 3.2118e+03, 2.4591e+03, 9.1788e-01, 0.0000e+00],\n",
      "        [1.0938e+03, 2.7266e+03, 3.2094e+03, 2.8675e+03, 9.0566e-01, 0.0000e+00],\n",
      "        [1.0733e+03, 1.2192e+03, 3.2259e+03, 1.3541e+03, 8.9568e-01, 0.0000e+00]], device='cuda:0')\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (4523, 3325)\n",
      "shape: torch.Size([24, 6])\n",
      "xywh: tensor([[2160.6648, 3322.6909, 2171.6763,  187.7664],\n",
      "        [2169.5469, 3189.5698, 2178.4004,  171.0518],\n",
      "        [2167.3850, 1943.4351, 2123.3662,  207.4573],\n",
      "        [2126.9363,  627.2997, 2169.1802,  252.0201],\n",
      "        [2170.3733, 2640.6333, 2148.8296,  181.6155],\n",
      "        [2124.3564, 2501.6914, 2101.3865,  210.6624],\n",
      "        [2152.1360, 3732.7915, 2197.9009,  161.8318],\n",
      "        [2173.5674, 2990.7778, 2160.3455,  175.9485],\n",
      "        [2168.8325, 2241.4053, 2138.7422,  203.4324],\n",
      "        [2162.4849, 1424.2866, 2166.8926,  201.3236],\n",
      "        [2108.0024,  738.5474, 2103.8472,  201.2244],\n",
      "        [2090.6404, 1839.6614, 2009.6299,  151.9349],\n",
      "        [2148.7925, 3587.6929, 2192.7385,  208.6531],\n",
      "        [2142.1790, 1171.7662, 2146.2246,  132.6116],\n",
      "        [2161.1504, 1572.0774, 2173.4761,  136.4620],\n",
      "        [2130.0798, 2901.3433, 2084.9390,  188.5527],\n",
      "        [2165.8411, 1712.0325, 2170.6445,  140.7313],\n",
      "        [2068.6133, 1042.7737, 1997.4849,  128.5533],\n",
      "        [2150.5049, 3471.9055, 2199.7112,  171.1260],\n",
      "        [2087.7124,  901.0621, 2026.7656,  140.7736],\n",
      "        [2120.8110, 2120.4895, 2013.9900,  167.5771],\n",
      "        [2151.0244, 2388.8159, 2121.5674,  140.5608],\n",
      "        [2151.6228, 2797.0537, 2115.6221,  140.8528],\n",
      "        [2149.5664, 1286.6417, 2152.6230,  134.9219]], device='cuda:0')\n",
      "xywhn: tensor([[0.6498, 0.7346, 0.6531, 0.0415],\n",
      "        [0.6525, 0.7052, 0.6552, 0.0378],\n",
      "        [0.6518, 0.4297, 0.6386, 0.0459],\n",
      "        [0.6397, 0.1387, 0.6524, 0.0557],\n",
      "        [0.6527, 0.5838, 0.6463, 0.0402],\n",
      "        [0.6389, 0.5531, 0.6320, 0.0466],\n",
      "        [0.6473, 0.8253, 0.6610, 0.0358],\n",
      "        [0.6537, 0.6612, 0.6497, 0.0389],\n",
      "        [0.6523, 0.4956, 0.6432, 0.0450],\n",
      "        [0.6504, 0.3149, 0.6517, 0.0445],\n",
      "        [0.6340, 0.1633, 0.6327, 0.0445],\n",
      "        [0.6288, 0.4067, 0.6044, 0.0336],\n",
      "        [0.6463, 0.7932, 0.6595, 0.0461],\n",
      "        [0.6443, 0.2591, 0.6455, 0.0293],\n",
      "        [0.6500, 0.3476, 0.6537, 0.0302],\n",
      "        [0.6406, 0.6415, 0.6270, 0.0417],\n",
      "        [0.6514, 0.3785, 0.6528, 0.0311],\n",
      "        [0.6221, 0.2305, 0.6007, 0.0284],\n",
      "        [0.6468, 0.7676, 0.6616, 0.0378],\n",
      "        [0.6279, 0.1992, 0.6096, 0.0311],\n",
      "        [0.6378, 0.4688, 0.6057, 0.0371],\n",
      "        [0.6469, 0.5281, 0.6381, 0.0311],\n",
      "        [0.6471, 0.6184, 0.6363, 0.0311],\n",
      "        [0.6465, 0.2845, 0.6474, 0.0298]], device='cuda:0')\n",
      "xyxy: tensor([[1074.8268, 3228.8079, 3246.5029, 3416.5742],\n",
      "        [1080.3466, 3104.0439, 3258.7471, 3275.0957],\n",
      "        [1105.7019, 1839.7064, 3229.0681, 2047.1637],\n",
      "        [1042.3461,  501.2897, 3211.5264,  753.3098],\n",
      "        [1095.9586, 2549.8257, 3244.7881, 2731.4412],\n",
      "        [1073.6631, 2396.3601, 3175.0496, 2607.0225],\n",
      "        [1053.1857, 3651.8757, 3251.0864, 3813.7075],\n",
      "        [1093.3945, 2902.8035, 3253.7400, 3078.7520],\n",
      "        [1099.4614, 2139.6890, 3238.2036, 2343.1213],\n",
      "        [1079.0386, 1323.6249, 3245.9312, 1524.9485],\n",
      "        [1056.0790,  637.9352, 3159.9260,  839.1596],\n",
      "        [1085.8254, 1763.6938, 3095.4553, 1915.6288],\n",
      "        [1052.4231, 3483.3662, 3245.1616, 3692.0193],\n",
      "        [1069.0667, 1105.4604, 3215.2913, 1238.0720],\n",
      "        [1074.4124, 1503.8464, 3247.8884, 1640.3085],\n",
      "        [1087.6105, 2807.0669, 3172.5493, 2995.6196],\n",
      "        [1080.5188, 1641.6669, 3251.1633, 1782.3982],\n",
      "        [1069.8708,  978.4969, 3067.3557, 1107.0503],\n",
      "        [1050.6494, 3386.3425, 3250.3606, 3557.4685],\n",
      "        [1074.3296,  830.6753, 3101.0952,  971.4489],\n",
      "        [1113.8162, 2036.7009, 3127.8062, 2204.2781],\n",
      "        [1090.2406, 2318.5354, 3211.8081, 2459.0962],\n",
      "        [1093.8116, 2726.6274, 3209.4338, 2867.4802],\n",
      "        [1073.2550, 1219.1808, 3225.8779, 1354.1027]], device='cuda:0')\n",
      "xyxyn: tensor([[0.3233, 0.7139, 0.9764, 0.7554],\n",
      "        [0.3249, 0.6863, 0.9801, 0.7241],\n",
      "        [0.3325, 0.4067, 0.9711, 0.4526],\n",
      "        [0.3135, 0.1108, 0.9659, 0.1666],\n",
      "        [0.3296, 0.5637, 0.9759, 0.6039],\n",
      "        [0.3229, 0.5298, 0.9549, 0.5764],\n",
      "        [0.3167, 0.8074, 0.9778, 0.8432],\n",
      "        [0.3288, 0.6418, 0.9786, 0.6807],\n",
      "        [0.3307, 0.4731, 0.9739, 0.5180],\n",
      "        [0.3245, 0.2926, 0.9762, 0.3372],\n",
      "        [0.3176, 0.1410, 0.9504, 0.1855],\n",
      "        [0.3266, 0.3899, 0.9310, 0.4235],\n",
      "        [0.3165, 0.7701, 0.9760, 0.8163],\n",
      "        [0.3215, 0.2444, 0.9670, 0.2737],\n",
      "        [0.3231, 0.3325, 0.9768, 0.3627],\n",
      "        [0.3271, 0.6206, 0.9542, 0.6623],\n",
      "        [0.3250, 0.3630, 0.9778, 0.3941],\n",
      "        [0.3218, 0.2163, 0.9225, 0.2448],\n",
      "        [0.3160, 0.7487, 0.9776, 0.7865],\n",
      "        [0.3231, 0.1837, 0.9327, 0.2148],\n",
      "        [0.3350, 0.4503, 0.9407, 0.4873],\n",
      "        [0.3279, 0.5126, 0.9660, 0.5437],\n",
      "        [0.3290, 0.6028, 0.9652, 0.6340],\n",
      "        [0.3228, 0.2696, 0.9702, 0.2994]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageOps\n",
    "\n",
    "# img = Image.open(r\"../../data/raw/Распознавание текстов/Уставные грамоты в jpg (Просветов)/11227024_doc1.jpg\")\n",
    "img = Image.open(get_rand_image())\n",
    "img = ImageOps.exif_transpose(img)\n",
    "\n",
    "res = text_detector.predict([img])\n",
    "\n",
    "# Process results list\n",
    "for result in res:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    result.show()  # display to screen"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T21:34:22.816316Z",
     "start_time": "2024-03-11T21:34:16.093496Z"
    }
   },
   "id": "9a2f929a94efa7b6",
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
