{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:00:40.163003Z",
     "start_time": "2024-03-14T15:00:40.155993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from datasets import load_metric\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor\n",
    ")\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15947033864af6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T12:44:46.815960Z",
     "start_time": "2024-03-14T12:44:46.809976Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19570a706ccbd98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:14:41.580368Z",
     "start_time": "2024-03-14T15:14:41.562369Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextRecognizePipeline:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ocr_processor = TrOCRProcessor.from_pretrained(\"raxtemur/trocr-base-ru\")\n",
    "        # self.ocr_model = VisionEncoderDecoderModel.from_pretrained(\"../../models/text_recognizer/trocr_ru_pretrain_3epoch/\", local_files_only=True).to(device)\n",
    "\n",
    "        # self.ocr_processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-small-handwritten\")\n",
    "        # self.ocr_model = VisionEncoderDecoderModel.from_pretrained(\"../../models/text_recognizer/checkpoint-1152/\", local_files_only=True).to(device)\n",
    "        self.ocr_model = VisionEncoderDecoderModel.from_pretrained(\"/storage3/vadim/HTR-historical/models/recognizer/trocr_ru_pretrain_3epoch\", local_files_only=True).to(device)\n",
    "\n",
    "        self.ocr_model.eval()\n",
    "        \n",
    "        # self.detection_model = YOLO(\"../../models/new_text_detector/best.pt\").to(device)\n",
    "        # self.detection_model = YOLO(\"/media/admin01/storage1/vadim/Historical-docs-OCR/models/text_detector/best_1024.pt\").to(device)\n",
    "        self.detection_model = YOLO(\"/storage3/vadim/HTR-historical/models/detector/best.pt\").to(device)\n",
    "\n",
    "        self.iou_threshold = 0.7\n",
    "        \n",
    "        # metrics\n",
    "        self.iou_list = []\n",
    "        self.cer_list = []\n",
    "        self.wer_list = []\n",
    "        \n",
    "        # Set special tokens used for creating the decoder_input_ids from the labels.\n",
    "        self.ocr_model.config.decoder_start_token_id = self.ocr_processor.tokenizer.cls_token_id\n",
    "        self.ocr_model.config.pad_token_id = self.ocr_processor.tokenizer.pad_token_id\n",
    "        # Set Correct vocab size.\n",
    "        self.ocr_model.config.vocab_size = self.ocr_model.config.decoder.vocab_size\n",
    "        self.ocr_model.config.eos_token_id = self.ocr_processor.tokenizer.sep_token_id\n",
    "        \n",
    "        self.ocr_model.config.max_length = 64\n",
    "        self.ocr_model.config.early_stopping = True\n",
    "        self.ocr_model.config.no_repeat_ngram_size = 3\n",
    "        self.ocr_model.config.length_penalty = 2.0\n",
    "        self.ocr_model.config.num_beams = 4\n",
    "    \n",
    "    def get_detections_and_crop_boxes(self, img: Image) -> list[Image]:\n",
    "        \n",
    "        def sort_bbox_by_y(bbox_list):\n",
    "            sorted_bbox = sorted(bbox_list, key=lambda bbox: (bbox[1], bbox[0]))  # Сортировка по координате y, затем по x\n",
    "            return sorted_bbox\n",
    "        \n",
    "        result = []\n",
    "        for predict, image in zip(self.detection_model.predict([img], verbose=False), [img]):\n",
    "            bboxes = predict.boxes.xyxy.cpu().tolist()\n",
    "            sorted_bboxes = sort_bbox_by_y(bboxes)\n",
    "            for box in sorted_bboxes:\n",
    "                cropped_image = image.crop(box)\n",
    "                result.append(cropped_image.convert(\"RGB\"))\n",
    "        return result\n",
    "    \n",
    "    def get_ocr_predictions(self, img_list: list[Image]) -> list[str]:\n",
    "        with torch.no_grad():\n",
    "            pixel_values = self.ocr_processor(img_list, return_tensors=\"pt\").pixel_values.to(device)\n",
    "            generated_ids = self.ocr_model.generate(pixel_values)\n",
    "            generated_text = self.ocr_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "        return generated_text\n",
    "    \n",
    "    def recognize(self, img_list: list[Image]) -> list[str]:\n",
    "        cropped_images = self.get_detections_and_crop_boxes(img_list)\n",
    "        recognized_text = self.get_ocr_predictions(cropped_images)\n",
    "        return recognized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665832d634a749c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:14:42.607213Z",
     "start_time": "2024-03-14T15:14:42.601210Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def get_rand_image():\n",
    "    path = pathlib.Path(\"../../data/processed/3 Production/text_detector/test/images\")\n",
    "    img_path = np.random.choice(list(path.iterdir()))\n",
    "    img = Image.open(img_path)\n",
    "    img = ImageOps.exif_transpose(img)\n",
    "    return img, img_path\n",
    "\n",
    "def get_label_text(data: pd.DataFrame, filename: str) -> list[str]:\n",
    "    return data[data[\"file_name\"].str.contains(filename)][\"text\"].to_list()\n",
    "\n",
    "def extract_filename(filename):\n",
    "    base_name, extension = os.path.splitext(filename)\n",
    "    parts = base_name.split(\"___\")\n",
    "    return parts[0] + extension\n",
    "\n",
    "def get_image(img_path: str | pathlib.Path) -> Image:\n",
    "    image = Image.open(img_path)\n",
    "    image = ImageOps.exif_transpose(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a48ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c4ece25c3e1a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:14:43.665427Z",
     "start_time": "2024-03-14T15:14:43.633421Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../../data/processed/3 Production/test.csv\", index_col=0)\n",
    "# data['file_name'] = data['file_name'].apply(lambda x: x.replace('.JPG', '.jpg'))\n",
    "\n",
    "# 0 - Губернаторские отчёты\n",
    "# 1 - Уставные грамоты – Афанасенков\n",
    "# 2 - Уставные грамоты в jpg (Просветов)\n",
    "# 3 - Победоносцев\n",
    "\n",
    "# отделяем губернаторские отчёты и уставные грамоты\n",
    "governors_reports = data[data[\"label\"] == 0]\n",
    "charter_letters = data[(data[\"label\"] == 1) | (data[\"label\"] == 2)]\n",
    "segment_annotation = data[data[\"label\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a858755",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c09d2fb31e851c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:14:55.131232Z",
     "start_time": "2024-03-14T15:14:46.265664Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ocr_pipeline = TextRecognizePipeline()\n",
    "\n",
    "cer_metric = load_metric(\"cer\", trust_remote_code=True)\n",
    "wer_metric = load_metric(\"wer\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b3d3d62f5f5e7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Подсчёт CER/WER для губернаторских отчётов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bb145a8289156",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "governors_reports[\"file_name\"] = governors_reports[\"file_name\"].apply(extract_filename)\n",
    "filenames = list(governors_reports.file_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec190014e66a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:15:48.512036Z",
     "start_time": "2024-03-14T15:14:58.588703Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_root_path = pathlib.Path(\"../../data/processed/3 Production/text_detector/test/images\")\n",
    "\n",
    "cer = []\n",
    "wer = []\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    file_path = image_root_path / pathlib.Path(filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        file_path = image_root_path / pathlib.Path(filename.replace('.JPG', '.jpg'))\n",
    "        \n",
    "    img = get_image(file_path)\n",
    "        \n",
    "    pred_text = ocr_pipeline.recognize(img)\n",
    "    pred_text = \" \".join(pred_text)\n",
    "    \n",
    "    label_text = get_label_text(governors_reports, filename)\n",
    "    label_text = \" \".join(label_text)\n",
    "    \n",
    "    cer.append(\n",
    "        cer_metric.compute(predictions=[pred_text], \n",
    "                           references=[label_text])\n",
    "    )\n",
    "    \n",
    "    wer.append(\n",
    "        wer_metric.compute(predictions=[pred_text], \n",
    "                           references=[label_text])\n",
    "    )\n",
    "\n",
    "print(f\"CER: {np.mean(cer)} | WER: {np.mean(wer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0601357b38dca76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Подсчёт CER/WER для отчётных грамот"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efa7743cbef235",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "charter_letters[\"file_name\"] = charter_letters[\"file_name\"].apply(extract_filename)\n",
    "filenames = list(charter_letters.file_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd4c6fef8aa7af8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T15:20:33.242025Z",
     "start_time": "2024-03-14T15:19:47.844191Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_root_path = pathlib.Path(\"../../data/processed/3 Production/text_detector/test/images\")\n",
    "\n",
    "cer = []\n",
    "wer = []\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    file_path = image_root_path / pathlib.Path(filename)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        file_path = image_root_path / pathlib.Path(filename.replace('.JPG', '.jpg'))\n",
    "    \n",
    "    img = get_image(file_path)\n",
    "        \n",
    "    pred_text = ocr_pipeline.recognize(img)\n",
    "    pred_text = \" \".join(pred_text)\n",
    "    \n",
    "    label_text = get_label_text(charter_letters, filename)\n",
    "    label_text = \" \".join(label_text)\n",
    "    \n",
    "    cer.append(\n",
    "        cer_metric.compute(predictions=[pred_text], references=[label_text])\n",
    "    )\n",
    "    \n",
    "    wer.append(\n",
    "        wer_metric.compute(predictions=[pred_text], references=[label_text])\n",
    "    )\n",
    "    \n",
    "print(f\"CER: {np.mean(cer)} | WER: {np.mean(wer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301bad6",
   "metadata": {},
   "source": [
    "Подсчёт CER/WER для 'Победоносцев' (резметка сегментами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27300096",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_annotation[\"file_name\"] = segment_annotation[\"file_name\"].apply(extract_filename)\n",
    "filenames = list(segment_annotation.file_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098827f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_root_path = pathlib.Path(\"../../data/processed/3 Production/text_detector/test/images\")\n",
    "\n",
    "cer = []\n",
    "wer = []\n",
    "\n",
    "for filename in tqdm(filenames, total=len(filenames)):\n",
    "    file_path = image_root_path / pathlib.Path(filename)\n",
    "    img = get_image(file_path)\n",
    "    \n",
    "    pred_text = ocr_pipeline.recognize(img)\n",
    "    pred_text = \" \".join(pred_text)\n",
    "    \n",
    "    label_text = get_label_text(segment_annotation, filename)\n",
    "    label_text = \" \".join(label_text)\n",
    "    \n",
    "    cer.append(\n",
    "        cer_metric.compute(predictions=[pred_text], references=[label_text])\n",
    "    )\n",
    "    \n",
    "    wer.append(\n",
    "        wer_metric.compute(predictions=[pred_text], references=[label_text])\n",
    "    )\n",
    "\n",
    "print(f\"CER: {np.mean(cer)} | WER: {np.mean(wer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed1565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887fef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('label')['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29965b2d",
   "metadata": {},
   "source": [
    "#TODO: Добавить выгрузку по bbox распознавание + разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6ce25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
