{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:18:41.460110900Z",
     "start_time": "2024-02-22T00:18:41.437588300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5aa94b8b4378922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:18:41.461111400Z",
     "start_time": "2024-02-22T00:18:41.447110600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322468a5c9ba8255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:18:41.477111Z",
     "start_time": "2024-02-22T00:18:41.462111900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE: int = 64\n",
    "    EPOCHS: int = 30\n",
    "    LEARNING_RATE: float = 0.00005\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    # DATA_ROOT: str = '../../data/processed/2 For OCR'\n",
    "    DATA_ROOT: str = '/media/admin01/storage1/vadim/Historical-docs-OCR/data/processed/3 Production'\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    # MODEL_NAME: str = 'microsoft/trocr-small-printed'\n",
    "    MODEL_NAME: str = 'microsoft/trocr-small-handwritten'\n",
    "    # MODEL_NAME: str = 'raxtemur/trocr-base-ru'\n",
    "    # MODEL_NAME: str = 'microsoft/trocr-small-stage1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4c315fe44e5677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:21:25.745892Z",
     "start_time": "2024-02-22T00:21:25.678927600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Размер обучающей выборки: 23609 | Размер тестовой выборки: 5849'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'train.csv'), index_col=0\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'valid.csv'), index_col=0\n",
    ")\n",
    "\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "train_df = train_df[train_df['text'] != 'unlabelled']\n",
    "test_df = test_df[test_df['text'] != 'unlabelled']\n",
    "\n",
    "f\"Размер обучающей выборки: {len(train_df)} | Размер тестовой выборки: {len(test_df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa23242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_5827___0.JPG</td>\n",
       "      <td>1001 томъ; Пронской 452 названiя</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_5827___1.JPG</td>\n",
       "      <td>921 томъ; Ряжской 410 названiй</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_5827___2.JPG</td>\n",
       "      <td>634 тома и Скопинской 401 названiе</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_5827___3.JPG</td>\n",
       "      <td>783 тома. Пожертвовано въ пользу</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_5827___4.JPG</td>\n",
       "      <td>библiотекъ Раненбургской 49 р. и</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23643</th>\n",
       "      <td>11227470_doc1___17.jpg</td>\n",
       "      <td>тиромъ на выкопировке съ плна</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23644</th>\n",
       "      <td>11227470_doc1___18.jpg</td>\n",
       "      <td>шета, которая имеетъ быть переда</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23645</th>\n",
       "      <td>11227470_doc1___19.jpg</td>\n",
       "      <td>на Г. Мировому Посреднику. Но,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23646</th>\n",
       "      <td>11227470_doc1___20.jpg</td>\n",
       "      <td>независимо сего, на основанiи ст.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23647</th>\n",
       "      <td>11227470_doc1___21.jpg</td>\n",
       "      <td>49 помянутаго Положенiя</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23609 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file_name                                text  label\n",
       "0            IMG_5827___0.JPG    1001 томъ; Пронской 452 названiя      0\n",
       "1            IMG_5827___1.JPG      921 томъ; Ряжской 410 названiй      0\n",
       "2            IMG_5827___2.JPG  634 тома и Скопинской 401 названiе      0\n",
       "3            IMG_5827___3.JPG    783 тома. Пожертвовано въ пользу      0\n",
       "4            IMG_5827___4.JPG    библiотекъ Раненбургской 49 р. и      0\n",
       "...                       ...                                 ...    ...\n",
       "23643  11227470_doc1___17.jpg       тиромъ на выкопировке съ плна      0\n",
       "23644  11227470_doc1___18.jpg    шета, которая имеетъ быть переда      0\n",
       "23645  11227470_doc1___19.jpg      на Г. Мировому Посреднику. Но,      0\n",
       "23646  11227470_doc1___20.jpg   независимо сего, на основанiи ст.      0\n",
       "23647  11227470_doc1___21.jpg             49 помянутаго Положенiя      0\n",
       "\n",
       "[23609 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c615f878a14f0a75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:40.356703600Z",
     "start_time": "2024-02-22T00:33:40.344757400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augmentations.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332aa388004276e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:42.031643300Z",
     "start_time": "2024-02-22T00:33:42.014123200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomOCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The image file name.\n",
    "        file_name = self.df['file_name'].iloc[idx]\n",
    "        # The text (label).\n",
    "        text = self.df['text'].iloc[idx]\n",
    "        # Read the image, apply augmentations, and get the transformed pixels.\n",
    "        image = Image.open(self.root_dir + file_name).convert('RGB')\n",
    "        \n",
    "        image = train_transforms(image)\n",
    "        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n",
    "        # Pass the text through the tokenizer and get the labels,\n",
    "        # i.e. tokenized labels.\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        # We are using -100 as the padding token.\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd940008fbcbfe04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:43.424304400Z",
     "start_time": "2024-02-22T00:33:42.504290100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "train_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer/train/'),\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "valid_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer/valid/'),\n",
    "    df=test_df,\n",
    "    processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f405273944bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:44.778432100Z",
     "start_time": "2024-02-22T00:33:43.424304400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel(\n",
      "  (encoder): DeiTModel(\n",
      "    (embeddings): DeiTEmbeddings(\n",
      "      (patch_embeddings): DeiTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): DeiTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DeiTLayer(\n",
      "          (attention): DeiTAttention(\n",
      "            (attention): DeiTSelfAttention(\n",
      "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): DeiTSelfOutput(\n",
      "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DeiTIntermediate(\n",
      "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DeiTOutput(\n",
      "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): DeiTPooler(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): TrOCRForCausalLM(\n",
      "    (model): TrOCRDecoderWrapper(\n",
      "      (decoder): TrOCRDecoder(\n",
      "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
      "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
      "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x TrOCRDecoderLayer(\n",
      "            (self_attn): TrOCRAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): TrOCRAttention(\n",
      "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
      "  )\n",
      ")\n",
      "61,596,672 total parameters.\n",
      "61,596,672 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "model.to(device)\n",
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2885dabfab723aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:44.808431700Z",
     "start_time": "2024-02-22T00:33:44.778432100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set special tokens used for creating the decoder_input_ids from the labels.\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# Set Correct vocab size.\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ecf33a3293c8d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:44.940564800Z",
     "start_time": "2024-02-22T00:33:44.928564900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ca4d8efb8be9a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:47.083917600Z",
     "start_time": "2024-02-22T00:33:45.671130700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_967810/2184949582.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load_metric(\"cer\", trust_remote_code=True)\n",
    "wer_metric = load_metric(\"wer\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a627db38bf825d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:47.098184600Z",
     "start_time": "2024-02-22T00:33:47.083917600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=http://localhost:8080\n",
      "env: CLEARML_API_HOST=http://localhost:8008\n",
      "env: CLEARML_FILES_HOST=http://localhost:8081\n",
      "env: CLEARML_API_ACCESS_KEY=LOIP4T1VXIPLP16VZJR9\n",
      "env: CLEARML_API_SECRET_KEY=RYVetvGfembTTfDKxnlWaXVWc60XWWka2WjNeRlczJmV5k2mgt\n"
     ]
    }
   ],
   "source": [
    "# если есть ClearML, то укажите свои настройки для логирования обучения детектора текста\n",
    "# с инструкцией, как поднять собственный ClearML, можно ознакомиться тут: https://github.com/allegroai/clearml-server \n",
    "%env CLEARML_WEB_HOST=http://localhost:8080\n",
    "%env CLEARML_API_HOST=http://localhost:8008\n",
    "%env CLEARML_FILES_HOST=http://localhost:8081\n",
    "%env CLEARML_API_ACCESS_KEY=LOIP4T1VXIPLP16VZJR9\n",
    "%env CLEARML_API_SECRET_KEY=RYVetvGfembTTfDKxnlWaXVWc60XWWka2WjNeRlczJmV5k2mgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d213f94178e9263d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:47.131182800Z",
     "start_time": "2024-02-22T00:33:47.100184700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    output_dir='seq2seq_model_checkpoints/',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    report_to='clearml',\n",
    "    num_train_epochs=TrainingConfig.EPOCHS,\n",
    "    dataloader_num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d1b4e32630c170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:47.665804900Z",
     "start_time": "2024-02-22T00:33:47.649783300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/transformers/models/trocr/processing_trocr.py:136: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d8b04cda4d1510a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=57e787289ffd4a9bb5775cec5bd56d55\n",
      "2024-03-19 00:54:48,514 - clearml.Task - INFO - Storing jupyter notebook directly as code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported key of type '<class 'int'>' found when connecting dictionary. It will be converted to str\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: http://localhost:8080/projects/4ee2d7e866464b1995dd559ab5add5f7/experiments/57e787289ffd4a9bb5775cec5bd56d55/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5550' max='5550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5550/5550 5:36:51, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.132600</td>\n",
       "      <td>3.452236</td>\n",
       "      <td>1.112115</td>\n",
       "      <td>1.468708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.381500</td>\n",
       "      <td>3.068127</td>\n",
       "      <td>0.960886</td>\n",
       "      <td>1.102586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.106600</td>\n",
       "      <td>2.860540</td>\n",
       "      <td>0.895642</td>\n",
       "      <td>1.062962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.921200</td>\n",
       "      <td>2.655235</td>\n",
       "      <td>0.867623</td>\n",
       "      <td>1.081888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.568100</td>\n",
       "      <td>2.134887</td>\n",
       "      <td>0.737278</td>\n",
       "      <td>0.949970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.889000</td>\n",
       "      <td>1.373746</td>\n",
       "      <td>0.458928</td>\n",
       "      <td>0.775675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.246200</td>\n",
       "      <td>0.938605</td>\n",
       "      <td>0.259292</td>\n",
       "      <td>0.613821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.848600</td>\n",
       "      <td>0.662828</td>\n",
       "      <td>0.178883</td>\n",
       "      <td>0.498115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.532609</td>\n",
       "      <td>0.140488</td>\n",
       "      <td>0.426746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.473127</td>\n",
       "      <td>0.126709</td>\n",
       "      <td>0.397451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.427040</td>\n",
       "      <td>0.111813</td>\n",
       "      <td>0.366347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.386800</td>\n",
       "      <td>0.400285</td>\n",
       "      <td>0.105471</td>\n",
       "      <td>0.351304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.383799</td>\n",
       "      <td>0.101122</td>\n",
       "      <td>0.338825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>0.365328</td>\n",
       "      <td>0.093776</td>\n",
       "      <td>0.325102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.347051</td>\n",
       "      <td>0.088463</td>\n",
       "      <td>0.316393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.341015</td>\n",
       "      <td>0.087458</td>\n",
       "      <td>0.309305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.338578</td>\n",
       "      <td>0.085146</td>\n",
       "      <td>0.306251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.332052</td>\n",
       "      <td>0.082869</td>\n",
       "      <td>0.297316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.331975</td>\n",
       "      <td>0.082458</td>\n",
       "      <td>0.301274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.324015</td>\n",
       "      <td>0.079542</td>\n",
       "      <td>0.291811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.080672</td>\n",
       "      <td>0.294639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.323506</td>\n",
       "      <td>0.078320</td>\n",
       "      <td>0.286458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>0.322909</td>\n",
       "      <td>0.077253</td>\n",
       "      <td>0.286307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.325071</td>\n",
       "      <td>0.076790</td>\n",
       "      <td>0.284007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.324436</td>\n",
       "      <td>0.076043</td>\n",
       "      <td>0.283366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.076088</td>\n",
       "      <td>0.284874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.328294</td>\n",
       "      <td>0.075455</td>\n",
       "      <td>0.283781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.326954</td>\n",
       "      <td>0.075124</td>\n",
       "      <td>0.280689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.328395</td>\n",
       "      <td>0.074353</td>\n",
       "      <td>0.281029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.329062</td>\n",
       "      <td>0.075609</td>\n",
       "      <td>0.282574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: http://localhost:8080/projects/4ee2d7e866464b1995dd559ab5add5f7/experiments/57e787289ffd4a9bb5775cec5bd56d55/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    }
   ],
   "source": [
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1530a4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511216b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca7613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
