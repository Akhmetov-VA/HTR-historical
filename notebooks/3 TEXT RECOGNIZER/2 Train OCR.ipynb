{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:18:41.460110900Z",
     "start_time": "2024-02-22T00:18:41.437588300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5aa94b8b4378922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:18:41.461111400Z",
     "start_time": "2024-02-22T00:18:41.447110600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322468a5c9ba8255",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:18:41.477111Z",
     "start_time": "2024-02-22T00:18:41.462111900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE: int = 64\n",
    "    EPOCHS: int = 30\n",
    "    LEARNING_RATE: float = 0.00005\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    # DATA_ROOT: str = '../../data/processed/2 For OCR'\n",
    "    DATA_ROOT: str = '/media/admin01/storage1/vadim/Historical-docs-OCR/data/processed/3 Production'\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    # MODEL_NAME: str = 'microsoft/trocr-small-printed'\n",
    "    MODEL_NAME: str = 'microsoft/trocr-small-handwritten'\n",
    "    # MODEL_NAME: str = 'raxtemur/trocr-base-ru'\n",
    "    # MODEL_NAME: str = 'microsoft/trocr-small-stage1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc4c315fe44e5677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:21:25.745892Z",
     "start_time": "2024-02-22T00:21:25.678927600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: 23609 | –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: 5849'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'train.csv'), index_col=0\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'valid.csv'), index_col=0\n",
    ")\n",
    "\n",
    "train_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "train_df = train_df[train_df['text'] != 'unlabelled']\n",
    "test_df = test_df[test_df['text'] != 'unlabelled']\n",
    "\n",
    "f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_df)} | –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa23242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG_5827___0.JPG</td>\n",
       "      <td>1001 —Ç–æ–º—ä; –ü—Ä–æ–Ω—Å–∫–æ–π 452 –Ω–∞–∑–≤–∞–Ωi—è</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG_5827___1.JPG</td>\n",
       "      <td>921 —Ç–æ–º—ä; –†—è–∂—Å–∫–æ–π 410 –Ω–∞–∑–≤–∞–Ωi–π</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG_5827___2.JPG</td>\n",
       "      <td>634 —Ç–æ–º–∞ –∏ –°–∫–æ–ø–∏–Ω—Å–∫–æ–π 401 –Ω–∞–∑–≤–∞–Ωi–µ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG_5827___3.JPG</td>\n",
       "      <td>783 —Ç–æ–º–∞. –ü–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–æ –≤—ä –ø–æ–ª—å–∑—É</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG_5827___4.JPG</td>\n",
       "      <td>–±–∏–±–ªi–æ—Ç–µ–∫—ä –†–∞–Ω–µ–Ω–±—É—Ä–≥—Å–∫–æ–π 49 —Ä. –∏</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23643</th>\n",
       "      <td>11227470_doc1___17.jpg</td>\n",
       "      <td>—Ç–∏—Ä–æ–º—ä –Ω–∞ –≤—ã–∫–æ–ø–∏—Ä–æ–≤–∫–µ —Å—ä –ø–ª–Ω–∞</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23644</th>\n",
       "      <td>11227470_doc1___18.jpg</td>\n",
       "      <td>—à–µ—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –∏–º–µ–µ—Ç—ä –±—ã—Ç—å –ø–µ—Ä–µ–¥–∞</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23645</th>\n",
       "      <td>11227470_doc1___19.jpg</td>\n",
       "      <td>–Ω–∞ –ì. –ú–∏—Ä–æ–≤–æ–º—É –ü–æ—Å—Ä–µ–¥–Ω–∏–∫—É. –ù–æ,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23646</th>\n",
       "      <td>11227470_doc1___20.jpg</td>\n",
       "      <td>–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ —Å–µ–≥–æ, –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ωi–∏ —Å—Ç.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23647</th>\n",
       "      <td>11227470_doc1___21.jpg</td>\n",
       "      <td>49 –ø–æ–º—è–Ω—É—Ç–∞–≥–æ –ü–æ–ª–æ–∂–µ–Ωi—è</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23609 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    file_name                                text  label\n",
       "0            IMG_5827___0.JPG    1001 —Ç–æ–º—ä; –ü—Ä–æ–Ω—Å–∫–æ–π 452 –Ω–∞–∑–≤–∞–Ωi—è      0\n",
       "1            IMG_5827___1.JPG      921 —Ç–æ–º—ä; –†—è–∂—Å–∫–æ–π 410 –Ω–∞–∑–≤–∞–Ωi–π      0\n",
       "2            IMG_5827___2.JPG  634 —Ç–æ–º–∞ –∏ –°–∫–æ–ø–∏–Ω—Å–∫–æ–π 401 –Ω–∞–∑–≤–∞–Ωi–µ      0\n",
       "3            IMG_5827___3.JPG    783 —Ç–æ–º–∞. –ü–æ–∂–µ—Ä—Ç–≤–æ–≤–∞–Ω–æ –≤—ä –ø–æ–ª—å–∑—É      0\n",
       "4            IMG_5827___4.JPG    –±–∏–±–ªi–æ—Ç–µ–∫—ä –†–∞–Ω–µ–Ω–±—É—Ä–≥—Å–∫–æ–π 49 —Ä. –∏      0\n",
       "...                       ...                                 ...    ...\n",
       "23643  11227470_doc1___17.jpg       —Ç–∏—Ä–æ–º—ä –Ω–∞ –≤—ã–∫–æ–ø–∏—Ä–æ–≤–∫–µ —Å—ä –ø–ª–Ω–∞      0\n",
       "23644  11227470_doc1___18.jpg    —à–µ—Ç–∞, –∫–æ—Ç–æ—Ä–∞—è –∏–º–µ–µ—Ç—ä –±—ã—Ç—å –ø–µ—Ä–µ–¥–∞      0\n",
       "23645  11227470_doc1___19.jpg      –Ω–∞ –ì. –ú–∏—Ä–æ–≤–æ–º—É –ü–æ—Å—Ä–µ–¥–Ω–∏–∫—É. –ù–æ,      0\n",
       "23646  11227470_doc1___20.jpg   –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ —Å–µ–≥–æ, –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ωi–∏ —Å—Ç.      0\n",
       "23647  11227470_doc1___21.jpg             49 –ø–æ–º—è–Ω—É—Ç–∞–≥–æ –ü–æ–ª–æ–∂–µ–Ωi—è      0\n",
       "\n",
       "[23609 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c615f878a14f0a75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:40.356703600Z",
     "start_time": "2024-02-22T00:33:40.344757400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augmentations.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332aa388004276e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:42.031643300Z",
     "start_time": "2024-02-22T00:33:42.014123200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomOCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The image file name.\n",
    "        file_name = self.df['file_name'].iloc[idx]\n",
    "        # The text (label).\n",
    "        text = self.df['text'].iloc[idx]\n",
    "        # Read the image, apply augmentations, and get the transformed pixels.\n",
    "        image = Image.open(self.root_dir + file_name).convert('RGB')\n",
    "        \n",
    "        image = train_transforms(image)\n",
    "        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n",
    "        # Pass the text through the tokenizer and get the labels,\n",
    "        # i.e. tokenized labels.\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        # We are using -100 as the padding token.\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd940008fbcbfe04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:43.424304400Z",
     "start_time": "2024-02-22T00:33:42.504290100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "train_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer/train/'),\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "valid_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer/valid/'),\n",
    "    df=test_df,\n",
    "    processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f405273944bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:44.778432100Z",
     "start_time": "2024-02-22T00:33:43.424304400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel(\n",
      "  (encoder): DeiTModel(\n",
      "    (embeddings): DeiTEmbeddings(\n",
      "      (patch_embeddings): DeiTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): DeiTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DeiTLayer(\n",
      "          (attention): DeiTAttention(\n",
      "            (attention): DeiTSelfAttention(\n",
      "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): DeiTSelfOutput(\n",
      "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DeiTIntermediate(\n",
      "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DeiTOutput(\n",
      "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): DeiTPooler(\n",
      "      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): TrOCRForCausalLM(\n",
      "    (model): TrOCRDecoderWrapper(\n",
      "      (decoder): TrOCRDecoder(\n",
      "        (embed_tokens): Embedding(64044, 256, padding_idx=1)\n",
      "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)\n",
      "        (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x TrOCRDecoderLayer(\n",
      "            (self_attn): TrOCRAttention(\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (activation_fn): ReLU()\n",
      "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): TrOCRAttention(\n",
      "              (k_proj): Linear(in_features=384, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=384, out_features=256, bias=True)\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=256, out_features=64044, bias=False)\n",
      "  )\n",
      ")\n",
      "61,596,672 total parameters.\n",
      "61,596,672 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "model.to(device)\n",
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2885dabfab723aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:44.808431700Z",
     "start_time": "2024-02-22T00:33:44.778432100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set special tokens used for creating the decoder_input_ids from the labels.\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# Set Correct vocab size.\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ecf33a3293c8d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:44.940564800Z",
     "start_time": "2024-02-22T00:33:44.928564900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ca4d8efb8be9a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:47.083917600Z",
     "start_time": "2024-02-22T00:33:45.671130700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_967810/2184949582.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load_metric(\"cer\", trust_remote_code=True)\n",
    "wer_metric = load_metric(\"wer\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a627db38bf825d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:47.098184600Z",
     "start_time": "2024-02-22T00:33:47.083917600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=http://localhost:8080\n",
      "env: CLEARML_API_HOST=http://localhost:8008\n",
      "env: CLEARML_FILES_HOST=http://localhost:8081\n",
      "env: CLEARML_API_ACCESS_KEY=LOIP4T1VXIPLP16VZJR9\n",
      "env: CLEARML_API_SECRET_KEY=RYVetvGfembTTfDKxnlWaXVWc60XWWka2WjNeRlczJmV5k2mgt\n"
     ]
    }
   ],
   "source": [
    "# –µ—Å–ª–∏ –µ—Å—Ç—å ClearML, —Ç–æ —É–∫–∞–∂–∏—Ç–µ —Å–≤–æ–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "# —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π, –∫–∞–∫ –ø–æ–¥–Ω—è—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π ClearML, –º–æ–∂–Ω–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Ç—É—Ç: https://github.com/allegroai/clearml-server \n",
    "%env CLEARML_WEB_HOST=http://localhost:8080\n",
    "%env CLEARML_API_HOST=http://localhost:8008\n",
    "%env CLEARML_FILES_HOST=http://localhost:8081\n",
    "%env CLEARML_API_ACCESS_KEY=LOIP4T1VXIPLP16VZJR9\n",
    "%env CLEARML_API_SECRET_KEY=RYVetvGfembTTfDKxnlWaXVWc60XWWka2WjNeRlczJmV5k2mgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d213f94178e9263d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:47.131182800Z",
     "start_time": "2024-02-22T00:33:47.100184700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    output_dir='seq2seq_model_checkpoints/',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    report_to='clearml',\n",
    "    num_train_epochs=TrainingConfig.EPOCHS,\n",
    "    dataloader_num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33d1b4e32630c170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T00:33:47.665804900Z",
     "start_time": "2024-02-22T00:33:47.649783300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/transformers/models/trocr/processing_trocr.py:136: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d8b04cda4d1510a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=57e787289ffd4a9bb5775cec5bd56d55\n",
      "2024-03-19 00:54:48,514 - clearml.Task - INFO - Storing jupyter notebook directly as code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported key of type '<class 'int'>' found when connecting dictionary. It will be converted to str\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: http://localhost:8080/projects/4ee2d7e866464b1995dd559ab5add5f7/experiments/57e787289ffd4a9bb5775cec5bd56d55/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5550' max='5550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5550/5550 5:36:51, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.132600</td>\n",
       "      <td>3.452236</td>\n",
       "      <td>1.112115</td>\n",
       "      <td>1.468708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.381500</td>\n",
       "      <td>3.068127</td>\n",
       "      <td>0.960886</td>\n",
       "      <td>1.102586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.106600</td>\n",
       "      <td>2.860540</td>\n",
       "      <td>0.895642</td>\n",
       "      <td>1.062962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.921200</td>\n",
       "      <td>2.655235</td>\n",
       "      <td>0.867623</td>\n",
       "      <td>1.081888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.568100</td>\n",
       "      <td>2.134887</td>\n",
       "      <td>0.737278</td>\n",
       "      <td>0.949970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.889000</td>\n",
       "      <td>1.373746</td>\n",
       "      <td>0.458928</td>\n",
       "      <td>0.775675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.246200</td>\n",
       "      <td>0.938605</td>\n",
       "      <td>0.259292</td>\n",
       "      <td>0.613821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.848600</td>\n",
       "      <td>0.662828</td>\n",
       "      <td>0.178883</td>\n",
       "      <td>0.498115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.532609</td>\n",
       "      <td>0.140488</td>\n",
       "      <td>0.426746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.523400</td>\n",
       "      <td>0.473127</td>\n",
       "      <td>0.126709</td>\n",
       "      <td>0.397451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.427040</td>\n",
       "      <td>0.111813</td>\n",
       "      <td>0.366347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.386800</td>\n",
       "      <td>0.400285</td>\n",
       "      <td>0.105471</td>\n",
       "      <td>0.351304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.343400</td>\n",
       "      <td>0.383799</td>\n",
       "      <td>0.101122</td>\n",
       "      <td>0.338825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.309600</td>\n",
       "      <td>0.365328</td>\n",
       "      <td>0.093776</td>\n",
       "      <td>0.325102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.347051</td>\n",
       "      <td>0.088463</td>\n",
       "      <td>0.316393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.341015</td>\n",
       "      <td>0.087458</td>\n",
       "      <td>0.309305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.233700</td>\n",
       "      <td>0.338578</td>\n",
       "      <td>0.085146</td>\n",
       "      <td>0.306251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.332052</td>\n",
       "      <td>0.082869</td>\n",
       "      <td>0.297316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.197500</td>\n",
       "      <td>0.331975</td>\n",
       "      <td>0.082458</td>\n",
       "      <td>0.301274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.182600</td>\n",
       "      <td>0.324015</td>\n",
       "      <td>0.079542</td>\n",
       "      <td>0.291811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.080672</td>\n",
       "      <td>0.294639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.323506</td>\n",
       "      <td>0.078320</td>\n",
       "      <td>0.286458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.147500</td>\n",
       "      <td>0.322909</td>\n",
       "      <td>0.077253</td>\n",
       "      <td>0.286307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.325071</td>\n",
       "      <td>0.076790</td>\n",
       "      <td>0.284007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.129800</td>\n",
       "      <td>0.324436</td>\n",
       "      <td>0.076043</td>\n",
       "      <td>0.283366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.327951</td>\n",
       "      <td>0.076088</td>\n",
       "      <td>0.284874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.116200</td>\n",
       "      <td>0.328294</td>\n",
       "      <td>0.075455</td>\n",
       "      <td>0.283781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>0.326954</td>\n",
       "      <td>0.075124</td>\n",
       "      <td>0.280689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.328395</td>\n",
       "      <td>0.074353</td>\n",
       "      <td>0.281029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.329062</td>\n",
       "      <td>0.075609</td>\n",
       "      <td>0.282574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: http://localhost:8080/projects/4ee2d7e866464b1995dd559ab5add5f7/experiments/57e787289ffd4a9bb5775cec5bd56d55/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    }
   ],
   "source": [
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1530a4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511216b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca7613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
