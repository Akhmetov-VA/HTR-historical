{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ef7ed2f7cbdac5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0626a38b147a9c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE: int = 16\n",
    "    EPOCHS: int = 5\n",
    "    LEARNING_RATE: float = 0.00005\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    DATA_ROOT: str = '../../data/processed/4 Segmenter test/'\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    # MODEL_NAME: str = 'microsoft/trocr-small-printed'\n",
    "    # MODEL_NAME: str = 'microsoft/trocr-small-handwritten'\n",
    "    MODEL_NAME: str = 'raxtemur/trocr-base-ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3665321d3bc54ee1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Размер обучающей выборки: 957 | Размер валидационной выборки: 263 | Размер тестовой выборки: 325'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'train.csv'), index_col=0\n",
    ").reset_index()\n",
    "\n",
    "valid_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'valid.csv'), index_col=0\n",
    ").reset_index()\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'test.csv'), index_col=0\n",
    ").reset_index()\n",
    "\n",
    "f\"Размер обучающей выборки: {len(train_df)} | Размер валидационной выборки: {len(valid_df)} | Размер тестовой выборки: {len(test_df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a1942b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Размер обучающей выборки: 899 | Размер валидационной выборки: 228 | Размер тестовой выборки: 325'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "valid_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "train_df = train_df[train_df['text'] != '.']\n",
    "valid_df = valid_df[valid_df['text'] != '.']\n",
    "test_df = test_df[test_df['text'] != '.']\n",
    "\n",
    "f\"Размер обучающей выборки: {len(train_df)} | Размер валидационной выборки: {len(valid_df)} | Размер тестовой выборки: {len(test_df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed8d56822b31993",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augmentations.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4bca3886e9e04e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomOCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The image file name.\n",
    "        file_name = self.df['file_name'].iloc[idx]\n",
    "        # The text (label).\n",
    "        text = self.df['text'].iloc[idx]\n",
    "        # Read the image, apply augmentations, and get the transformed pixels.\n",
    "        image = Image.open(self.root_dir + file_name).convert('RGB')\n",
    "        image = train_transforms(image)\n",
    "        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n",
    "        # Pass the text through the tokenizer and get the labels,\n",
    "        # i.e. tokenized labels.\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        # We are using -100 as the padding token.\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0e15a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_fdf0882e-20.png</td>\n",
       "      <td>1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_fdf0882e-20.png</td>\n",
       "      <td>Мартъ. Вышелъ 2 томъ курса гр. права.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_fdf0882e-20.png</td>\n",
       "      <td>16 Мая Свадьба Кати Пеликанъ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_fdf0882e-20.png</td>\n",
       "      <td>22 – 26. въ Москве.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_fdf0882e-20.png</td>\n",
       "      <td>4 Iюня. Уехали заграницу съ Соничкой и съ М. Е.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>31_10a09237-0191.jpeg</td>\n",
       "      <td>Счастье, счастье увиделъ я на лице у</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>32_10a09237-0191.jpeg</td>\n",
       "      <td>милой моей Катюши!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>33_10a09237-0191.jpeg</td>\n",
       "      <td>Мы скрываемся ото всехъ днемъ, а вечеромъ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>34_10a09237-0191.jpeg</td>\n",
       "      <td>въ темноте – прогулки къ нашему бревнышку.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>35_10a09237-0191.jpeg</td>\n",
       "      <td>14 Iюля рано утромъ прiезжаетъ Ал. андр</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>899 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file_name                                             text\n",
       "0        0_fdf0882e-20.png                                             1875\n",
       "1        1_fdf0882e-20.png            Мартъ. Вышелъ 2 томъ курса гр. права.\n",
       "2        2_fdf0882e-20.png                    16 Мая Свадьба Кати Пеликанъ.\n",
       "3        3_fdf0882e-20.png                              22 – 26. въ Москве.\n",
       "4        4_fdf0882e-20.png  4 Iюня. Уехали заграницу съ Соничкой и съ М. Е.\n",
       "..                     ...                                              ...\n",
       "952  31_10a09237-0191.jpeg             Счастье, счастье увиделъ я на лице у\n",
       "953  32_10a09237-0191.jpeg                               милой моей Катюши!\n",
       "954  33_10a09237-0191.jpeg        Мы скрываемся ото всехъ днемъ, а вечеромъ\n",
       "955  34_10a09237-0191.jpeg       въ темноте – прогулки къ нашему бревнышку.\n",
       "956  35_10a09237-0191.jpeg          14 Iюля рано утромъ прiезжаетъ Ал. андр\n",
       "\n",
       "[899 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "407c9c6de066fe98",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "train_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer', 'train/'),\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "valid_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer', 'valid/'),\n",
    "    df=valid_df,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "test_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer', 'test/'),\n",
    "    df=test_df,\n",
    "    processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79bed1c018423f79",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel(\n",
      "  (encoder): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): ViTPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): TrOCRForCausalLM(\n",
      "    (model): TrOCRDecoderWrapper(\n",
      "      (decoder): TrOCRDecoder(\n",
      "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
      "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
      "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x TrOCRDecoderLayer(\n",
      "            (self_attn): TrOCRAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (activation_fn): GELUActivation()\n",
      "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): TrOCRAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
      "  )\n",
      ")\n",
      "333,921,792 total parameters.\n",
      "333,921,792 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "model.to(device)\n",
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d0c522ea81f1a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set special tokens used for creating the decoder_input_ids from the labels.\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# Set Correct vocab size.\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc4b069e9ec809f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46c1c96849ac9b8c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2964131/2184949582.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load_metric(\"cer\", trust_remote_code=True)\n",
    "wer_metric = load_metric(\"wer\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de33197c410a0fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=http://localhost:8080\n",
      "env: CLEARML_API_HOST=http://localhost:8008\n",
      "env: CLEARML_FILES_HOST=http://localhost:8081\n",
      "env: CLEARML_API_ACCESS_KEY=LOIP4T1VXIPLP16VZJR9\n",
      "env: CLEARML_API_SECRET_KEY=RYVetvGfembTTfDKxnlWaXVWc60XWWka2WjNeRlczJmV5k2mgt\n"
     ]
    }
   ],
   "source": [
    "# если есть ClearML, то укажите свои настройки для логирования обучения детектора текста\n",
    "# с инструкцией, как поднять собственный ClearML, можно ознакомиться тут: https://github.com/allegroai/clearml-server \n",
    "%env CLEARML_WEB_HOST=http://localhost:8080\n",
    "%env CLEARML_API_HOST=http://localhost:8008\n",
    "%env CLEARML_FILES_HOST=http://localhost:8081\n",
    "%env CLEARML_API_ACCESS_KEY=LOIP4T1VXIPLP16VZJR9\n",
    "%env CLEARML_API_SECRET_KEY=RYVetvGfembTTfDKxnlWaXVWc60XWWka2WjNeRlczJmV5k2mgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12be829d1d32c9f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    output_dir='seq2seq_model_checkpoints/',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    report_to='clearml',\n",
    "    num_train_epochs=TrainingConfig.EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c58846ba7d874027",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/transformers/models/trocr/processing_trocr.py:136: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ff99c830d3d798c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=ed0dc172068f4599bf844c666e67a278\n",
      "2024-03-28 16:05:59,009 - clearml.Task - INFO - Storing jupyter notebook directly as code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported key of type '<class 'int'>' found when connecting dictionary. It will be converted to str\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: http://localhost:8080/projects/4ee2d7e866464b1995dd559ab5add5f7/experiments/ed0dc172068f4599bf844c666e67a278/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145/145 18:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.184500</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.293156</td>\n",
       "      <td>0.671480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.429812</td>\n",
       "      <td>0.218776</td>\n",
       "      <td>0.550903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.393281</td>\n",
       "      <td>0.204967</td>\n",
       "      <td>0.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.374909</td>\n",
       "      <td>0.193701</td>\n",
       "      <td>0.506859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.378726</td>\n",
       "      <td>0.190672</td>\n",
       "      <td>0.510469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "Removed shared tensor {'decoder.output_projection.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    }
   ],
   "source": [
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9d9f9",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b0f247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4248153865337372,\n",
       " 'eval_cer': 0.19709905443863032,\n",
       " 'eval_wer': 0.4975793437331899,\n",
       " 'eval_runtime': 222.8998,\n",
       " 'eval_samples_per_second': 1.458,\n",
       " 'eval_steps_per_second': 0.049,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3307944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# Example for a sequence-to-sequence task\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "decoded_predictions = [processor.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in predictions.predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4697dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pred'] = decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7ebeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('pobed_seg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d38937f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_909ccbb0-18.png</td>\n",
       "      <td>Начало общества у В.К. Константина. Отделъ Общ...</td>\n",
       "      <td>На что общества у В. К. Насильичина. Отделъ Об...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_909ccbb0-18.png</td>\n",
       "      <td>Валуевъ – М-ръ Госуд. имуществъ.</td>\n",
       "      <td>Валуевъ – М заъ Госуда имущества.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_909ccbb0-18.png</td>\n",
       "      <td>20 мая. Поездка съ Катей и Соничкой черезъ Москву</td>\n",
       "      <td>28. Мая. Поездка съ Котей и Соничкой чередъ  М...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_909ccbb0-18.png</td>\n",
       "      <td>въ Смоленскъ. у а. в. шевандиной и у Дiодора</td>\n",
       "      <td>въ смоленiя. у А. в. Шевандиной и утодора</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_909ccbb0-18.png</td>\n",
       "      <td>въ Александровскомъ. вернулись 1 Iюня.</td>\n",
       "      <td>въ Александровскомъ. вернулись 17юня.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>37_c6d63ae5-15.png</td>\n",
       "      <td>Варшаву и Берлинъ, и Парижъ и Лондонъ,</td>\n",
       "      <td>Варшаву и берлинъ, и \"парижъ и-Лондоне,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>38_c6d63ae5-15.png</td>\n",
       "      <td>на о-въ Вайтъ. – Шенклинъ. На обратномъ</td>\n",
       "      <td>на О въ Войте. – Шенклинъ. На обратность</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>39_c6d63ae5-15.png</td>\n",
       "      <td>Пути черезъ Ломжу – возвращаемся</td>\n",
       "      <td>пути черезъ Ломжу – возвращаемся</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>40_c6d63ae5-15.png</td>\n",
       "      <td>1 Сентября.</td>\n",
       "      <td>1 Сентябрь.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>41_c6d63ae5-15.png</td>\n",
       "      <td>Переехали на Спасскую ул. д. Дементьева</td>\n",
       "      <td>переехали на Спасскую ул. д. Дементьевъ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name                                               text  \\\n",
       "0     0_909ccbb0-18.png  Начало общества у В.К. Константина. Отделъ Общ...   \n",
       "1     1_909ccbb0-18.png                   Валуевъ – М-ръ Госуд. имуществъ.   \n",
       "2     2_909ccbb0-18.png  20 мая. Поездка съ Катей и Соничкой черезъ Москву   \n",
       "3     3_909ccbb0-18.png       въ Смоленскъ. у а. в. шевандиной и у Дiодора   \n",
       "4     4_909ccbb0-18.png             въ Александровскомъ. вернулись 1 Iюня.   \n",
       "..                  ...                                                ...   \n",
       "320  37_c6d63ae5-15.png             Варшаву и Берлинъ, и Парижъ и Лондонъ,   \n",
       "321  38_c6d63ae5-15.png           на о-въ Вайтъ. – Шенклинъ. На обратномъ    \n",
       "322  39_c6d63ae5-15.png                   Пути черезъ Ломжу – возвращаемся   \n",
       "323  40_c6d63ae5-15.png                                        1 Сентября.   \n",
       "324  41_c6d63ae5-15.png            Переехали на Спасскую ул. д. Дементьева   \n",
       "\n",
       "                                                  pred  \n",
       "0    На что общества у В. К. Насильичина. Отделъ Об...  \n",
       "1                    Валуевъ – М заъ Госуда имущества.  \n",
       "2    28. Мая. Поездка съ Котей и Соничкой чередъ  М...  \n",
       "3            въ смоленiя. у А. в. Шевандиной и утодора  \n",
       "4                въ Александровскомъ. вернулись 17юня.  \n",
       "..                                                 ...  \n",
       "320            Варшаву и берлинъ, и \"парижъ и-Лондоне,  \n",
       "321           на О въ Войте. – Шенклинъ. На обратность  \n",
       "322                   пути черезъ Ломжу – возвращаемся  \n",
       "323                                        1 Сентябрь.  \n",
       "324            переехали на Спасскую ул. д. Дементьевъ  \n",
       "\n",
       "[325 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc44a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
