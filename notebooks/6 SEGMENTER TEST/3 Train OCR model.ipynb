{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from datasets import load_metric\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ef7ed2f7cbdac5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0626a38b147a9c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE: int = 16\n",
    "    EPOCHS: int = 5\n",
    "    LEARNING_RATE: float = 0.00005\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    DATA_ROOT: str = '../../data/processed/4 Segmenter test/'\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    # MODEL_NAME: str = 'microsoft/trocr-small-printed'\n",
    "    # MODEL_NAME: str = 'microsoft/trocr-small-handwritten'\n",
    "    MODEL_NAME: str = 'raxtemur/trocr-base-ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3665321d3bc54ee1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: 957 | –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: 263 | –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: 325'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'train.csv'), index_col=0\n",
    ").reset_index()\n",
    "\n",
    "valid_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'valid.csv'), index_col=0\n",
    ").reset_index()\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    os.path.join(DatasetConfig.DATA_ROOT, 'test.csv'), index_col=0\n",
    ").reset_index()\n",
    "\n",
    "f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_df)} | –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(valid_df)} | –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9a1942b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: 899 | –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: 228 | –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: 325'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "valid_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "train_df = train_df[train_df['text'] != '.']\n",
    "valid_df = valid_df[valid_df['text'] != '.']\n",
    "test_df = test_df[test_df['text'] != '.']\n",
    "\n",
    "f\"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_df)} | –†–∞–∑–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(valid_df)} | –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_df)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed8d56822b31993",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augmentations.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f4bca3886e9e04e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomOCRDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The image file name.\n",
    "        file_name = self.df['file_name'].iloc[idx]\n",
    "        # The text (label).\n",
    "        text = self.df['text'].iloc[idx]\n",
    "        # Read the image, apply augmentations, and get the transformed pixels.\n",
    "        image = Image.open(self.root_dir + file_name).convert('RGB')\n",
    "        image = train_transforms(image)\n",
    "        pixel_values = self.processor(image, return_tensors='pt').pixel_values\n",
    "        # Pass the text through the tokenizer and get the labels,\n",
    "        # i.e. tokenized labels.\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "        # We are using -100 as the padding token.\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0e15a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_fdf0882e-20.png</td>\n",
       "      <td>1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_fdf0882e-20.png</td>\n",
       "      <td>–ú–∞—Ä—Ç—ä. –í—ã—à–µ–ª—ä 2 —Ç–æ–º—ä –∫—É—Ä—Å–∞ –≥—Ä. –ø—Ä–∞–≤–∞.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_fdf0882e-20.png</td>\n",
       "      <td>16 –ú–∞—è –°–≤–∞–¥—å–±–∞ –ö–∞—Ç–∏ –ü–µ–ª–∏–∫–∞–Ω—ä.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_fdf0882e-20.png</td>\n",
       "      <td>22 ‚Äì 26. –≤—ä –ú–æ—Å–∫–≤–µ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_fdf0882e-20.png</td>\n",
       "      <td>4 I—é–Ω—è. –£–µ—Ö–∞–ª–∏ –∑–∞–≥—Ä–∞–Ω–∏—Ü—É —Å—ä –°–æ–Ω–∏—á–∫–æ–π –∏ —Å—ä –ú. –ï.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>31_10a09237-0191.jpeg</td>\n",
       "      <td>–°—á–∞—Å—Ç—å–µ, —Å—á–∞—Å—Ç—å–µ —É–≤–∏–¥–µ–ª—ä —è –Ω–∞ –ª–∏—Ü–µ —É</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>32_10a09237-0191.jpeg</td>\n",
       "      <td>–º–∏–ª–æ–π –º–æ–µ–π –ö–∞—Ç—é—à–∏!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>33_10a09237-0191.jpeg</td>\n",
       "      <td>–ú—ã —Å–∫—Ä—ã–≤–∞–µ–º—Å—è –æ—Ç–æ –≤—Å–µ—Ö—ä –¥–Ω–µ–º—ä, –∞ –≤–µ—á–µ—Ä–æ–º—ä</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>34_10a09237-0191.jpeg</td>\n",
       "      <td>–≤—ä —Ç–µ–º–Ω–æ—Ç–µ ‚Äì –ø—Ä–æ–≥—É–ª–∫–∏ –∫—ä –Ω–∞—à–µ–º—É –±—Ä–µ–≤–Ω—ã—à–∫—É.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>35_10a09237-0191.jpeg</td>\n",
       "      <td>14 I—é–ª—è —Ä–∞–Ω–æ —É—Ç—Ä–æ–º—ä –ø—Äi–µ–∑–∂–∞–µ—Ç—ä –ê–ª. –∞–Ω–¥—Ä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>899 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file_name                                             text\n",
       "0        0_fdf0882e-20.png                                             1875\n",
       "1        1_fdf0882e-20.png            –ú–∞—Ä—Ç—ä. –í—ã—à–µ–ª—ä 2 —Ç–æ–º—ä –∫—É—Ä—Å–∞ –≥—Ä. –ø—Ä–∞–≤–∞.\n",
       "2        2_fdf0882e-20.png                    16 –ú–∞—è –°–≤–∞–¥—å–±–∞ –ö–∞—Ç–∏ –ü–µ–ª–∏–∫–∞–Ω—ä.\n",
       "3        3_fdf0882e-20.png                              22 ‚Äì 26. –≤—ä –ú–æ—Å–∫–≤–µ.\n",
       "4        4_fdf0882e-20.png  4 I—é–Ω—è. –£–µ—Ö–∞–ª–∏ –∑–∞–≥—Ä–∞–Ω–∏—Ü—É —Å—ä –°–æ–Ω–∏—á–∫–æ–π –∏ —Å—ä –ú. –ï.\n",
       "..                     ...                                              ...\n",
       "952  31_10a09237-0191.jpeg             –°—á–∞—Å—Ç—å–µ, —Å—á–∞—Å—Ç—å–µ —É–≤–∏–¥–µ–ª—ä —è –Ω–∞ –ª–∏—Ü–µ —É\n",
       "953  32_10a09237-0191.jpeg                               –º–∏–ª–æ–π –º–æ–µ–π –ö–∞—Ç—é—à–∏!\n",
       "954  33_10a09237-0191.jpeg        –ú—ã —Å–∫—Ä—ã–≤–∞–µ–º—Å—è –æ—Ç–æ –≤—Å–µ—Ö—ä –¥–Ω–µ–º—ä, –∞ –≤–µ—á–µ—Ä–æ–º—ä\n",
       "955  34_10a09237-0191.jpeg       –≤—ä —Ç–µ–º–Ω–æ—Ç–µ ‚Äì –ø—Ä–æ–≥—É–ª–∫–∏ –∫—ä –Ω–∞—à–µ–º—É –±—Ä–µ–≤–Ω—ã—à–∫—É.\n",
       "956  35_10a09237-0191.jpeg          14 I—é–ª—è —Ä–∞–Ω–æ —É—Ç—Ä–æ–º—ä –ø—Äi–µ–∑–∂–∞–µ—Ç—ä –ê–ª. –∞–Ω–¥—Ä\n",
       "\n",
       "[899 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "407c9c6de066fe98",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "train_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer', 'train/'),\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "valid_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer', 'valid/'),\n",
    "    df=valid_df,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "test_dataset = CustomOCRDataset(\n",
    "    root_dir=os.path.join(DatasetConfig.DATA_ROOT, 'text_recognizer', 'test/'),\n",
    "    df=test_df,\n",
    "    processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79bed1c018423f79",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel(\n",
      "  (encoder): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): ViTPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): TrOCRForCausalLM(\n",
      "    (model): TrOCRDecoderWrapper(\n",
      "      (decoder): TrOCRDecoder(\n",
      "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
      "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
      "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x TrOCRDecoderLayer(\n",
      "            (self_attn): TrOCRAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (activation_fn): GELUActivation()\n",
      "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): TrOCRAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
      "  )\n",
      ")\n",
      "333,921,792 total parameters.\n",
      "333,921,792 training parameters.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(ModelConfig.MODEL_NAME)\n",
    "model.to(device)\n",
    "print(model)\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d0c522ea81f1a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set special tokens used for creating the decoder_input_ids from the labels.\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# Set Correct vocab size.\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc4b069e9ec809f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), lr=TrainingConfig.LEARNING_RATE, weight_decay=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46c1c96849ac9b8c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2964131/2184949582.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\", trust_remote_code=True)\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load_metric(\"cer\", trust_remote_code=True)\n",
    "wer_metric = load_metric(\"wer\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer, \"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de33197c410a0fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=http://localhost:8080\n",
      "env: CLEARML_API_HOST=http://localhost:8008\n",
      "env: CLEARML_FILES_HOST=http://localhost:8081\n",
      "env: CLEARML_API_ACCESS_KEY=LOIP4T1VXIPLP16VZJR9\n",
      "env: CLEARML_API_SECRET_KEY=RYVetvGfembTTfDKxnlWaXVWc60XWWka2WjNeRlczJmV5k2mgt\n"
     ]
    }
   ],
   "source": [
    "# –µ—Å–ª–∏ –µ—Å—Ç—å ClearML, —Ç–æ —É–∫–∞–∂–∏—Ç–µ —Å–≤–æ–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "# —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π, –∫–∞–∫ –ø–æ–¥–Ω—è—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π ClearML, –º–æ–∂–Ω–æ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Ç—É—Ç: https://github.com/allegroai/clearml-server \n",
    "%env CLEARML_WEB_HOST=http://localhost:8080\n",
    "%env CLEARML_API_HOST=http://localhost:8008\n",
    "%env CLEARML_FILES_HOST=http://localhost:8081\n",
    "%env CLEARML_API_ACCESS_KEY=LOIP4T1VXIPLP16VZJR9\n",
    "%env CLEARML_API_SECRET_KEY=RYVetvGfembTTfDKxnlWaXVWc60XWWka2WjNeRlczJmV5k2mgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12be829d1d32c9f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=TrainingConfig.BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    output_dir='seq2seq_model_checkpoints/',\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    report_to='clearml',\n",
    "    num_train_epochs=TrainingConfig.EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c58846ba7d874027",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/transformers/models/trocr/processing_trocr.py:136: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer.\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ff99c830d3d798c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=ed0dc172068f4599bf844c666e67a278\n",
      "2024-03-28 16:05:59,009 - clearml.Task - INFO - Storing jupyter notebook directly as code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported key of type '<class 'int'>' found when connecting dictionary. It will be converted to str\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML results page: http://localhost:8080/projects/4ee2d7e866464b1995dd559ab5add5f7/experiments/ed0dc172068f4599bf844c666e67a278/output/log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145/145 18:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cer</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.184500</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.293156</td>\n",
       "      <td>0.671480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.429812</td>\n",
       "      <td>0.218776</td>\n",
       "      <td>0.550903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>0.393281</td>\n",
       "      <td>0.204967</td>\n",
       "      <td>0.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.374909</td>\n",
       "      <td>0.193701</td>\n",
       "      <td>0.506859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.378726</td>\n",
       "      <td>0.190672</td>\n",
       "      <td>0.510469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1295: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "Removed shared tensor {'decoder.output_projection.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n",
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\n"
     ]
    }
   ],
   "source": [
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9d9f9",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b0f247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4248153865337372,\n",
       " 'eval_cer': 0.19709905443863032,\n",
       " 'eval_wer': 0.4975793437331899,\n",
       " 'eval_runtime': 222.8998,\n",
       " 'eval_samples_per_second': 1.458,\n",
       " 'eval_steps_per_second': 0.049,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3307944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/admin01/storage1/vadim/Historical-docs-OCR/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# Example for a sequence-to-sequence task\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "decoded_predictions = [processor.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in predictions.predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4697dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pred'] = decoded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7ebeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('pobed_seg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d38937f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_909ccbb0-18.png</td>\n",
       "      <td>–ù–∞—á–∞–ª–æ –æ–±—â–µ—Å—Ç–≤–∞ —É –í.–ö. –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–∞. –û—Ç–¥–µ–ª—ä –û–±—â...</td>\n",
       "      <td>–ù–∞ —á—Ç–æ –æ–±—â–µ—Å—Ç–≤–∞ —É –í. –ö. –ù–∞—Å–∏–ª—å–∏—á–∏–Ω–∞. –û—Ç–¥–µ–ª—ä –û–±...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_909ccbb0-18.png</td>\n",
       "      <td>–í–∞–ª—É–µ–≤—ä ‚Äì –ú-—Ä—ä –ì–æ—Å—É–¥. –∏–º—É—â–µ—Å—Ç–≤—ä.</td>\n",
       "      <td>–í–∞–ª—É–µ–≤—ä ‚Äì –ú –∑–∞—ä –ì–æ—Å—É–¥–∞ –∏–º—É—â–µ—Å—Ç–≤–∞.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_909ccbb0-18.png</td>\n",
       "      <td>20 –º–∞—è. –ü–æ–µ–∑–¥–∫–∞ —Å—ä –ö–∞—Ç–µ–π –∏ –°–æ–Ω–∏—á–∫–æ–π —á–µ—Ä–µ–∑—ä –ú–æ—Å–∫–≤—É</td>\n",
       "      <td>28. –ú–∞—è. –ü–æ–µ–∑–¥–∫–∞ —Å—ä –ö–æ—Ç–µ–π –∏ –°–æ–Ω–∏—á–∫–æ–π —á–µ—Ä–µ–¥—ä  –ú...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3_909ccbb0-18.png</td>\n",
       "      <td>–≤—ä –°–º–æ–ª–µ–Ω—Å–∫—ä. —É –∞. –≤. —à–µ–≤–∞–Ω–¥–∏–Ω–æ–π –∏ —É –îi–æ–¥–æ—Ä–∞</td>\n",
       "      <td>–≤—ä —Å–º–æ–ª–µ–Ωi—è. —É –ê. –≤. –®–µ–≤–∞–Ω–¥–∏–Ω–æ–π –∏ —É—Ç–æ–¥–æ—Ä–∞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4_909ccbb0-18.png</td>\n",
       "      <td>–≤—ä –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤—Å–∫–æ–º—ä. –≤–µ—Ä–Ω—É–ª–∏—Å—å 1 I—é–Ω—è.</td>\n",
       "      <td>–≤—ä –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤—Å–∫–æ–º—ä. –≤–µ—Ä–Ω—É–ª–∏—Å—å 17—é–Ω—è.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>37_c6d63ae5-15.png</td>\n",
       "      <td>–í–∞—Ä—à–∞–≤—É –∏ –ë–µ—Ä–ª–∏–Ω—ä, –∏ –ü–∞—Ä–∏–∂—ä –∏ –õ–æ–Ω–¥–æ–Ω—ä,</td>\n",
       "      <td>–í–∞—Ä—à–∞–≤—É –∏ –±–µ—Ä–ª–∏–Ω—ä, –∏ \"–ø–∞—Ä–∏–∂—ä –∏-–õ–æ–Ω–¥–æ–Ω–µ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>38_c6d63ae5-15.png</td>\n",
       "      <td>–Ω–∞ –æ-–≤—ä –í–∞–π—Ç—ä. ‚Äì –®–µ–Ω–∫–ª–∏–Ω—ä. –ù–∞ –æ–±—Ä–∞—Ç–Ω–æ–º—ä</td>\n",
       "      <td>–Ω–∞ –û –≤—ä –í–æ–π—Ç–µ. ‚Äì –®–µ–Ω–∫–ª–∏–Ω—ä. –ù–∞ –æ–±—Ä–∞—Ç–Ω–æ—Å—Ç—å</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>39_c6d63ae5-15.png</td>\n",
       "      <td>–ü—É—Ç–∏ —á–µ—Ä–µ–∑—ä –õ–æ–º–∂—É ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è</td>\n",
       "      <td>–ø—É—Ç–∏ —á–µ—Ä–µ–∑—ä –õ–æ–º–∂—É ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>40_c6d63ae5-15.png</td>\n",
       "      <td>1 –°–µ–Ω—Ç—è–±—Ä—è.</td>\n",
       "      <td>1 –°–µ–Ω—Ç—è–±—Ä—å.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>41_c6d63ae5-15.png</td>\n",
       "      <td>–ü–µ—Ä–µ–µ—Ö–∞–ª–∏ –Ω–∞ –°–ø–∞—Å—Å–∫—É—é —É–ª. –¥. –î–µ–º–µ–Ω—Ç—å–µ–≤–∞</td>\n",
       "      <td>–ø–µ—Ä–µ–µ—Ö–∞–ª–∏ –Ω–∞ –°–ø–∞—Å—Å–∫—É—é —É–ª. –¥. –î–µ–º–µ–Ω—Ç—å–µ–≤—ä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              file_name                                               text  \\\n",
       "0     0_909ccbb0-18.png  –ù–∞—á–∞–ª–æ –æ–±—â–µ—Å—Ç–≤–∞ —É –í.–ö. –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–∞. –û—Ç–¥–µ–ª—ä –û–±—â...   \n",
       "1     1_909ccbb0-18.png                   –í–∞–ª—É–µ–≤—ä ‚Äì –ú-—Ä—ä –ì–æ—Å—É–¥. –∏–º—É—â–µ—Å—Ç–≤—ä.   \n",
       "2     2_909ccbb0-18.png  20 –º–∞—è. –ü–æ–µ–∑–¥–∫–∞ —Å—ä –ö–∞—Ç–µ–π –∏ –°–æ–Ω–∏—á–∫–æ–π —á–µ—Ä–µ–∑—ä –ú–æ—Å–∫–≤—É   \n",
       "3     3_909ccbb0-18.png       –≤—ä –°–º–æ–ª–µ–Ω—Å–∫—ä. —É –∞. –≤. —à–µ–≤–∞–Ω–¥–∏–Ω–æ–π –∏ —É –îi–æ–¥–æ—Ä–∞   \n",
       "4     4_909ccbb0-18.png             –≤—ä –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤—Å–∫–æ–º—ä. –≤–µ—Ä–Ω—É–ª–∏—Å—å 1 I—é–Ω—è.   \n",
       "..                  ...                                                ...   \n",
       "320  37_c6d63ae5-15.png             –í–∞—Ä—à–∞–≤—É –∏ –ë–µ—Ä–ª–∏–Ω—ä, –∏ –ü–∞—Ä–∏–∂—ä –∏ –õ–æ–Ω–¥–æ–Ω—ä,   \n",
       "321  38_c6d63ae5-15.png           –Ω–∞ –æ-–≤—ä –í–∞–π—Ç—ä. ‚Äì –®–µ–Ω–∫–ª–∏–Ω—ä. –ù–∞ –æ–±—Ä–∞—Ç–Ω–æ–º—ä    \n",
       "322  39_c6d63ae5-15.png                   –ü—É—Ç–∏ —á–µ—Ä–µ–∑—ä –õ–æ–º–∂—É ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è   \n",
       "323  40_c6d63ae5-15.png                                        1 –°–µ–Ω—Ç—è–±—Ä—è.   \n",
       "324  41_c6d63ae5-15.png            –ü–µ—Ä–µ–µ—Ö–∞–ª–∏ –Ω–∞ –°–ø–∞—Å—Å–∫—É—é —É–ª. –¥. –î–µ–º–µ–Ω—Ç—å–µ–≤–∞   \n",
       "\n",
       "                                                  pred  \n",
       "0    –ù–∞ —á—Ç–æ –æ–±—â–µ—Å—Ç–≤–∞ —É –í. –ö. –ù–∞—Å–∏–ª—å–∏—á–∏–Ω–∞. –û—Ç–¥–µ–ª—ä –û–±...  \n",
       "1                    –í–∞–ª—É–µ–≤—ä ‚Äì –ú –∑–∞—ä –ì–æ—Å—É–¥–∞ –∏–º—É—â–µ—Å—Ç–≤–∞.  \n",
       "2    28. –ú–∞—è. –ü–æ–µ–∑–¥–∫–∞ —Å—ä –ö–æ—Ç–µ–π –∏ –°–æ–Ω–∏—á–∫–æ–π —á–µ—Ä–µ–¥—ä  –ú...  \n",
       "3            –≤—ä —Å–º–æ–ª–µ–Ωi—è. —É –ê. –≤. –®–µ–≤–∞–Ω–¥–∏–Ω–æ–π –∏ —É—Ç–æ–¥–æ—Ä–∞  \n",
       "4                –≤—ä –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤—Å–∫–æ–º—ä. –≤–µ—Ä–Ω—É–ª–∏—Å—å 17—é–Ω—è.  \n",
       "..                                                 ...  \n",
       "320            –í–∞—Ä—à–∞–≤—É –∏ –±–µ—Ä–ª–∏–Ω—ä, –∏ \"–ø–∞—Ä–∏–∂—ä –∏-–õ–æ–Ω–¥–æ–Ω–µ,  \n",
       "321           –Ω–∞ –û –≤—ä –í–æ–π—Ç–µ. ‚Äì –®–µ–Ω–∫–ª–∏–Ω—ä. –ù–∞ –æ–±—Ä–∞—Ç–Ω–æ—Å—Ç—å  \n",
       "322                   –ø—É—Ç–∏ —á–µ—Ä–µ–∑—ä –õ–æ–º–∂—É ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è  \n",
       "323                                        1 –°–µ–Ω—Ç—è–±—Ä—å.  \n",
       "324            –ø–µ—Ä–µ–µ—Ö–∞–ª–∏ –Ω–∞ –°–ø–∞—Å—Å–∫—É—é —É–ª. –¥. –î–µ–º–µ–Ω—Ç—å–µ–≤—ä  \n",
       "\n",
       "[325 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc44a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
